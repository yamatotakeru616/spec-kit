{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Notebook E — 05_create_training_data.ipynb (Prepare Dataset)\n",
                "\n",
                "This notebook loads annotated point clouds (or labeled folders) and generates a training dataset (features + labels) for the classifier.\n",
                "\n",
                "## Concepts\n",
                "- **Features**: Geometric and color features extracted from points (XYZ, Normals, Colors, Height).\n",
                "- **Labels**: Ground truth class derived from `.las` classification or folder structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell E0 — Install dependencies\n",
                "!pip install numpy open3d trimesh laspy scipy onnxruntime numba py-vox-io"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell E1 — Mount Drive & Setup\n",
                "from google.colab import drive\n",
                "import os\n",
                "import sys\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "BASE = \"/content/drive/MyDrive/voxel_engine\"\n",
                "SRC_REPO = \"/content/spec-kit/src\"\n",
                "SRC_LEGACY = \"/content/voxel_engine_src\"\n",
                "\n",
                "INPUT_DIR = os.path.join(BASE, \"input\")\n",
                "TRAIN_DIR = os.path.join(BASE, \"training_data\")\n",
                "\n",
                "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
                "\n",
                "# Ensure src is in path\n",
                "if os.path.exists(SRC_REPO) and SRC_REPO not in sys.path:\n",
                "    sys.path.append(SRC_REPO)\n",
                "    print(f\"Added {SRC_REPO} to sys.path\")\n",
                "\n",
                "if os.path.exists(SRC_LEGACY) and SRC_LEGACY not in sys.path:\n",
                "    sys.path.append(SRC_LEGACY)\n",
                "    print(f\"Added {SRC_LEGACY} to sys.path\")\n",
                "\n",
                "# Reload modules to ensure we have latest version\n",
                "try:\n",
                "    import loader, classify, utils\n",
                "    import numpy as np\n",
                "    import imp\n",
                "    imp.reload(loader)\n",
                "    imp.reload(classify)\n",
                "    imp.reload(utils)\n",
                "except ImportError as e:\n",
                "    print(\"Error importing modules. Make sure you have cloned the repo or run previous notebooks.\")\n",
                "    print(\"Try running: !git clone https://github.com/yamatotakeru616/spec-kit.git /content/spec-kit\")\n",
                "    raise e\n",
                "\n",
                "print(\"Ready to process data in:\", INPUT_DIR)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell E2 — Define Feature Extraction Helper\n",
                "\n",
                "def process_file(path, label_override=None):\n",
                "    \"\"\"\n",
                "    Loads a file, computes features, and returns (features, labels).\n",
                "    If label_override is provided, forces all points to have that label.\n",
                "    Otherwise, tries to read labels from file (LAS classification).\n",
                "    \"\"\"\n",
                "    print(f\"Processing {os.path.basename(path)}...\")\n",
                "    \n",
                "    # Load with upgraded loader that supports labels\n",
                "    # Note: If loader.py wasn't updated in previous steps, this might fail to get labels if not using folder mode.\n",
                "    try:\n",
                "        if hasattr(loader, 'load_annotated_pointcloud'):\n",
                "            pts, cols, file_labels = loader.load_annotated_pointcloud(path)\n",
                "        else:\n",
                "            # Fallback if old loader\n",
                "            pts, cols = loader.load_pointcloud(path)\n",
                "            file_labels = None\n",
                "    except Exception as e:\n",
                "        print(f\"Failed to load {path}: {e}\")\n",
                "        return None, None\n",
                "\n",
                "    # Compute features\n",
                "    # compute_features(points, colors=None, normals=None)\n",
                "    feats = classify.compute_features(pts, colors=cols)\n",
                "    \n",
                "    # Determine Labels\n",
                "    target_labels = None\n",
                "    \n",
                "    if label_override is not None:\n",
                "        # Create full array of this label\n",
                "        target_labels = np.full(pts.shape[0], label_override, dtype=np.uint8)\n",
                "    elif file_labels is not None:\n",
                "        target_labels = file_labels\n",
                "    else:\n",
                "        print(f\"Warning: No labels found for {path} and no override provided. Skipping labels.\")\n",
                "    \n",
                "    return feats, target_labels"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell E3 — Process Dataset (Option A: Folders as Classes)\n",
                "# Structure:\n",
                "# input/\n",
                "#   class_1_ground/\n",
                "#     file1.laz\n",
                "#   class_2_building/\n",
                "#     file2.ply\n",
                "\n",
                "all_feats = []\n",
                "all_labels = []\n",
                "\n",
                "# Define your class mapping here if using folders\n",
                "CLASS_MAP = {\n",
                "    \"ground\": 1,\n",
                "    \"vegetation\": 2,\n",
                "    \"building\": 3,\n",
                "    \"vehicle\": 4\n",
                "}\n",
                "\n",
                "scan_mode = \"folder\" # or \"file\"\n",
                "\n",
                "if scan_mode == \"folder\":\n",
                "    for folder_name, label_id in CLASS_MAP.items():\n",
                "        folder_path = os.path.join(INPUT_DIR, folder_name)\n",
                "        if not os.path.exists(folder_path): \n",
                "            continue\n",
                "            \n",
                "        files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.las', '.laz', '.ply', '.obj'))]\n",
                "        print(f\"Found {len(files)} files for class '{folder_name}' (ID: {label_id})\")\n",
                "        \n",
                "        for f in files:\n",
                "            fp = os.path.join(folder_path, f)\n",
                "            f_feats, f_lbls = process_file(fp, label_override=label_id)\n",
                "            if f_feats is not None and f_lbls is not None:\n",
                "                all_feats.append(f_feats)\n",
                "                all_labels.append(f_lbls)\n",
                "\n",
                "# Cell E4 — Process Dataset (Option B: Single file with embedded labels)\n",
                "# If using a single labeled LAS file in input/\n",
                "if scan_mode == \"file\":\n",
                "     files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.las', '.laz'))]\n",
                "     for f in files:\n",
                "         fp = os.path.join(INPUT_DIR, f)\n",
                "         f_feats, f_lbls = process_file(fp)\n",
                "         if f_feats is not None and f_lbls is not None:\n",
                "            all_feats.append(f_feats)\n",
                "            all_labels.append(f_lbls)\n",
                "            \n",
                "if len(all_feats) > 0:\n",
                "    X = np.vstack(all_feats)\n",
                "    y = np.concatenate(all_labels)\n",
                "    print(f\"\\nTotal Training Data: {X.shape[0]} points\")\n",
                "    print(f\"Features: {X.shape[1]}\")\n",
                "    print(f\"Classes: {np.unique(y)}\")\n",
                "else:\n",
                "    print(\"No training data found.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell E5 — Save Training Data\n",
                "if len(all_feats) > 0:\n",
                "    save_path = os.path.join(TRAIN_DIR, \"training_data.npz\")\n",
                "    np.savez_compressed(save_path, X=X, y=y)\n",
                "    print(f\"Saved dataset to {save_path}\")\n",
                "    \n",
                "    # Optional: Save a small sample for verification\n",
                "    # sample_idx = np.random.choice(len(X), min(10000, len(X)), replace=False)\n",
                "    # np.savez(os.path.join(TRAIN_DIR, \"sample_data.npz\"), X=X[sample_idx], y=y[sample_idx])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}